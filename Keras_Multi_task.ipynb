{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Multi-task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNr/8O5LUJCoTjZYBsEkiT7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneesh2711/Bioinformatics/blob/master/Keras_Multi_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk-joHYscS5L"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import inf\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAhpEJCGz0C4",
        "outputId": "67d3637f-bbf1-495a-9c97-26f43c994c5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device_name = tensorflow.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-VUzC2Xs0Qz",
        "outputId": "dbbbc4a3-ac0a-4779-b2da-e4fe38ae50d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_fn = \"/content/drive/My Drive/Colab Notebooks/Protien_prediction/data\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNn8mvnsgYy9"
      },
      "source": [
        "branch_term = \"GO0071840\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PK-0eAremu3"
      },
      "source": [
        "def oversampleData(tasks,train_set):\n",
        "  train_set_final = {}\n",
        "  train_set_pos = {}\n",
        "  train_set_neg = {}\n",
        "  label_pos = {}\n",
        "  label_neg = {}\n",
        "  for task in tasks:\n",
        "    data_pos = train_set[task][0][train_set[task][1].ravel() == 1, :]\n",
        "    data_neg = train_set[task][0][train_set[task][1].ravel() == 0, :]\n",
        "    pos_len = len(data_pos)\n",
        "    #print(pos_len)\n",
        "    neg_len = len(data_neg)\n",
        "    #print(neg_len)\n",
        "    if pos_len<neg_len:\n",
        "      ratio = math.ceil(neg_len/pos_len)\n",
        "      #print(ratio)\n",
        "      data_pos = np.concatenate([data_pos]*ratio)\n",
        "      #print(len(data_pos))\n",
        "      data_pos = data_pos[0:neg_len][:]\n",
        "      #print(len(data_pos))\n",
        "    if pos_len>neg_len:\n",
        "      ratio = math.ceil(pos_len/neg_len)\n",
        "      #print(ratio)\n",
        "      data_neg = np.concatenate([data_neg]*ratio)\n",
        "      #print(len(data_neg))\n",
        "      data_neg = data_neg[0:pos_len][:]\n",
        "      #print(len(data_neg))\n",
        "    label_pos[task] = [1]*len(data_pos)\n",
        "    label_neg[task] = [0]*len(data_neg)\n",
        "    train_set_pos[task] = data_pos\n",
        "    train_set_neg[task] = data_neg\n",
        "    assert len(train_set_pos[task]) == len(train_set_neg[task])\n",
        "    assert len(label_pos[task]) == len(label_neg[task])\n",
        "    assert len(train_set_pos[task]) == len(label_pos[task])\n",
        "    assert len(train_set_neg[task]) == len(label_neg[task])\n",
        "    train_set_final[task] = [np.concatenate((train_set_pos[task],train_set_neg[task])), np.concatenate((label_pos[task],label_neg[task]))]\n",
        "  return train_set_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSDQPn_0dwhY"
      },
      "source": [
        "space = {'shared_layers_num': hp.choice('shared_layers_num', [1, 2, 3]),\n",
        "         'spec_layers_num': hp.choice('spec_layers_num', [1, 2, 3]),\n",
        "         'dropout_i': hp.uniform('dropout_i', .05, .95),\n",
        "         'dropout_o': hp.uniform('dropout_o', .05, .95),\n",
        "         'shared_hidden_number_1': hp.choice('shared_hidden_number_1', [256, 512, 768, 1024]),\n",
        "         'shared_hidden_number_2': hp.choice('shared_hidden_number_2', [256, 512, 768, 1024]),\n",
        "         'shared_hidden_number_3': hp.choice('shared_hidden_number_3', [256, 512, 768, 1024]),\n",
        "         'spec_hidden_number_1': hp.choice('spec_hidden_number_1', [64, 128, 256, 512]),\n",
        "         'spec_hidden_number_2': hp.choice('spec_hidden_number_2', [64, 128, 256, 512]),\n",
        "         'spec_hidden_number_3': hp.choice('spec_hidden_number_3', [64, 128, 256, 512]),\n",
        "         'batch_size': hp.choice('batch_size', [32, 64, 128, 256]),\n",
        "         'optimizer': hp.choice('optimizer', ['Nadam', 'Adagrad', 'Adadelta', 'Adam', 'Rmsprop']),\n",
        "         'branch_term': branch_term,\n",
        "         'loss': 'binary_crossentropy',\n",
        "         'oversample': hp.choice('oversample', [True,False]),\n",
        "         }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mzsy1NiOWAI"
      },
      "source": [
        "\n",
        "def fun(params):\n",
        "  metrics=['accuracy']\n",
        "  oversample = params['oversample']\n",
        "\n",
        "  shared_layers_num = params['shared_layers_num']\n",
        "  spec_layers_num = params['spec_layers_num']\n",
        "  shared_layers_units = [params['shared_hidden_number_1'],params['shared_hidden_number_2'],params['shared_hidden_number_3']]\n",
        "  spec_layers_units = [params['spec_hidden_number_1'],params['spec_hidden_number_2'],params['spec_hidden_number_3']]\n",
        "  dropout = [params['dropout_i'],params['dropout_o'],params['dropout_o']]\n",
        "  optimizer = params['optimizer']\n",
        "  loss= params['loss']\n",
        "  metrics=['accuracy']\n",
        "  batch_size = params['batch_size']\n",
        "  oversample = params['oversample']\n",
        "  branch_term = params['branch_term']\n",
        "\n",
        "  all_data_x_fn = data_fn + '/all_data_X.csv'\n",
        "  all_data_x = pd.read_csv(all_data_x_fn, sep='\\t', header=0, index_col=0)\n",
        "  all_proteins_train = [p.replace('\"', '') for p in all_data_x.index]\n",
        "  all_data_x.index = all_proteins_train\n",
        "\n",
        "  train_fn = data_fn + '/train_sets/' + branch_term + '_train.csv'\n",
        "  train_y = pd.read_csv(train_fn, sep='\\t', header=0, index_col=0)\n",
        "  proteins_train = [p for p in train_y.index if p in all_data_x.index]\n",
        "  train_x = all_data_x.loc[proteins_train, :]\n",
        "  train_y = train_y.loc[proteins_train, :]\n",
        "  validate_fn = data_fn + '/train_sets/' + branch_term + '_valid.csv'\n",
        "  validate_y = pd.read_csv(validate_fn, sep='\\t', header=0, index_col=0)\n",
        "  proteins_validate = [p for p in validate_y.index if p in all_data_x.index]\n",
        "  validate_x = all_data_x.loc[proteins_validate, :]\n",
        "  validate_y = validate_y.loc[proteins_validate, :]\n",
        "\n",
        "  tasks = train_y.columns\n",
        "\n",
        "  train_set = {}\n",
        "  validate_set = {}\n",
        "  for task in tasks:\n",
        "    proteins_in_task = train_y.index[train_y.loc[:, task] != inf]\n",
        "    train_set[task] = [train_x.loc[proteins_in_task, :].values, train_y.loc[proteins_in_task, task].values]\n",
        "    proteins_in_task = validate_y.index[validate_y.loc[:, task] != inf]\n",
        "    validate_set[task] = [validate_x.loc[proteins_in_task, :].values, validate_y.loc[proteins_in_task, task].values]\n",
        "\n",
        "  train_set_final = train_set\n",
        "  if oversample:\n",
        "    train_set_final = oversampleData(tasks,train_set)\n",
        "\n",
        "  shared_layers = []\n",
        "  spec_layers = {}\n",
        "  outputs = {}\n",
        "  models = {}\n",
        "\n",
        "  inputs = Input(shape=(258,))\n",
        "  x = Dropout(dropout[0])(inputs)\n",
        "  for i in range(shared_layers_num):\n",
        "    name = \"shared-\"+ str(i)\n",
        "    layer = Dense(shared_layers_units[i], activation='relu',name = name)(x)\n",
        "    shared_layers.append(layer)\n",
        "    x = Dropout(dropout[1])(layer)\n",
        "  for task in tasks:\n",
        "    spec_layers[task] = []\n",
        "    layer = Dense(spec_layers_units[0], activation='relu',name = task+\"-spec-0\")(x)\n",
        "    spec_layers[task].append(layer)\n",
        "    y = Dropout(dropout[2])(layer)\n",
        "    for i in range(1,spec_layers_num):\n",
        "      name = task+\"-spec-\"+ str(i)\n",
        "      layer = Dense(spec_layers_units[i], activation='relu',name = name)(y)\n",
        "      spec_layers[task].append(layer)\n",
        "      y = Dropout(dropout[2])(layer)\n",
        "    outputs[task] = Dense(1, activation='sigmoid',name=task)(y)\n",
        "    models[task] = Model(inputs = inputs,outputs = outputs[task],name = task)\n",
        "\n",
        "  for task in tasks:\n",
        "    models[task].compile(optimizer = optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "  callback = EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "  for task in tasks:\n",
        "    models[task].fit(train_set_final[task][0], train_set_final[task][1], epochs = 50,batch_size=batch_size, verbose=0, callbacks=[callback], validation_data = (validate_set[task][0], validate_set[task][1]))\n",
        "\n",
        "  for task in tasks:\n",
        "    for i in range(shared_layers_num):\n",
        "      name = \"shared-\"+str(i)\n",
        "      layer = models[task].get_layer(name)\n",
        "      if layer.trainable == True:\n",
        "        layer.trainable = False\n",
        "      assert layer.trainable == False\n",
        "    models[task].compile(optimizer = optimizer, loss= loss, metrics=metrics)\n",
        "\n",
        "  for task in tasks:\n",
        "    models[task].fit(train_set_final[task][0], train_set_final[task][1], epochs = 50,batch_size=batch_size, verbose=0, callbacks=[callback], validation_data = (validate_set[task][0], validate_set[task][1]))\n",
        "\n",
        "  test_fn = data_fn + '/train_sets/' + branch_term + '_test.csv'\n",
        "  test_y = pd.read_csv(test_fn, sep='\\t', header=0, index_col=0)\n",
        "  proteins_test = [p for p in test_y.index if p in all_data_x.index]\n",
        "  test_x = all_data_x.loc[proteins_test, :]\n",
        "  test_y = test_y.loc[proteins_test, :]\n",
        "  test_set = {}\n",
        "  predict_set = {}\n",
        "  test_labels = []\n",
        "  predict_labels = []\n",
        "  for task in tasks:\n",
        "    proteins_in_task = test_y.index[test_y.loc[:, task] != inf]\n",
        "    test_set[task] = [test_x.loc[proteins_in_task, :].values, test_y.loc[proteins_in_task, task].values]\n",
        "    predict_set[task] = models[task].predict(test_set[task][0])\n",
        "    test_labels = np.concatenate((test_labels,test_set[task][1]))\n",
        "    predict_set[task] = predict_set[task].reshape(len(predict_set[task]))\n",
        "    predict_labels = np.concatenate((predict_labels,predict_set[task]))\n",
        "\n",
        "  i = 1\n",
        "  Fmax = 0\n",
        "  T = 0\n",
        "  while i < 100:\n",
        "    k = i/100\n",
        "    predict_labels_T = (predict_labels>k)\n",
        "    f1 = f1_score(test_labels, predict_labels_T)\n",
        "    \n",
        "    if f1>Fmax:\n",
        "      Fmax = f1\n",
        "      T = i\n",
        "    i = i + 1\n",
        "  print(str(Fmax) + \"---\"+ str(T))\n",
        "  return {'loss': -Fmax, 'tres':T, 'status': STATUS_OK}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn9BWNf6o4vu",
        "outputId": "5d7e5309-6862-4f88-b40c-ee52c2ee81f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trials = Trials()\n",
        "best = fmin(fun, space, algo=tpe.suggest, max_evals=10, trials=trials,return_argmin=False)\n",
        "print('best: ', best)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3064929736383219---49\n",
            "0.3649795347908555---19\n",
            "0.362758326823379---20\n",
            "0.27646338241298973---49\n",
            "0.44014285714285717---32\n",
            "0.2677152132542259---49\n",
            "0.3607681755829904---25\n",
            "0.3619768778311135---39\n",
            "0.2590868321018238---47\n",
            "0.48664335143983933---34\n",
            "100%|██████████| 10/10 [25:21<00:00, 152.10s/it, best loss: -0.48664335143983933]\n",
            "best:  {'batch_size': 256, 'branch_term': 'GO0071840', 'dropout_i': 0.3243499474136037, 'dropout_o': 0.17749052057828274, 'loss': 'binary_crossentropy', 'optimizer': 'Rmsprop', 'oversample': False, 'shared_hidden_number_1': 512, 'shared_hidden_number_2': 512, 'shared_hidden_number_3': 1024, 'shared_layers_num': 1, 'spec_hidden_number_1': 256, 'spec_hidden_number_2': 512, 'spec_hidden_number_3': 128, 'spec_layers_num': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}